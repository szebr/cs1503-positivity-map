{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc416063",
   "metadata": {},
   "source": [
    "# Skills lab: Conditional Probabilities\n",
    "#### Sentiment analysis of tweets with Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3616a",
   "metadata": {},
   "source": [
    "The idea of summarizing and visualizing events and emotions of the entire world received a lot of attention in the Machine Learning community. As an example see how the [GDELT project](https://www.gdeltproject.org/) tried to convert world media and news into visualizations of the events and emotions: [LINK](https://firstmonday.org/ojs/index.php/fm/article/view/4366/3654) \n",
    "\n",
    "Our goal in this assignment is less extreme: we want to create a system for monitoring emotions of Twitter users and plot the positivity score for small geographic areas on Google maps.\n",
    "\n",
    "To classify each tweet we are going to use the Naive Bayes classifier, which, despite its “naive” assumptions, has great performance in classifying documents based on their word content.\n",
    "\n",
    "The main idea of the Document classification with Naive Bayes is as following:\n",
    "\n",
    "For each new document with words $Word_1 … Word_n$ and each category $c$ compute the probability:\n",
    "\n",
    "$P(Category = c | Word1 = true, …, Wordn = true) =\\alpha*P(Category = c) \\prod_{i=1}^n P(Wordi = true | Category = c)$\n",
    "\n",
    "Then a new document can be classified as belonging to a category $c_{best}$ for which the above probability is the highest:\n",
    "\n",
    "$c_{best} = argmax(c \\in C) P(Category = c | Word_1 = true, …, Word_n = true)$ \n",
    "\n",
    "Note, that we don't calculate any real probabilities here. Instead, we are estimating which class is more likely, given the evidence (words). This is another reason why Naïve Bayes is so robust: It is not so much interested in the real probabilities, but only determines which class is more likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a5d67",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Our training dataset was obtained from the Niek Sanders corpus, who has done an awesome job of **manually** labeling more than 5,000 tweets with 4 categories: Positive, Negative, Neutral, and Irrelevant. In general, you could use his script, [install.py](https://github.com/zfz/twitter_corpus.git), to fetch the corresponding Twitter text by tweet id. Because the script is playing nicely with Twitter's servers, it might take quite some time to download the data, so the dataset [full-corpus.csv](https://drive.google.com/file/d/1n3XFcUp8yoLf4BOj0rxEpTw1VCwyK1s-/view?usp=sharing) has been already pre-created for you by your hard-working instructor:). \n",
    "\n",
    "We also preprocessed the dataset, leaving only 2 relevant columns: the text of a twit, and the sentiment class. The preprocessed dataset is in file [labeled_corpus.tsv](https://drive.google.com/file/d/1NXvKTuTO6oNjonKkDm2DGdN5fqj-2RE7/view?usp=drive_link), and the tweet text is separated from the class label using tab (‘\\t’). That is why the file extension is tsv - which means tab-separated values (similar to csv - comma-separated values). This is our training dataset, used to estimate the probabilities of words given a category. Download the dataset [labeled_corpus.tsv](https://drive.google.com/file/d/1NXvKTuTO6oNjonKkDm2DGdN5fqj-2RE7/view?usp=drive_link), and place it in the same directory where your notebook is.\n",
    "\n",
    "Let's open the dataset and look at the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc2f8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now all @Apple has to do is get swype on the iphone and it will be crack. Iphone that is\n",
      "positive\n",
      "\n",
      "@Apple will be adding more carrier support to the iPhone 4S (just announced)\n",
      "positive\n",
      "\n",
      "Hilarious @youtube video - guy does a duet with @apple 's Siri. Pretty much sums up the love affair! http://t.co/8ExbnQjY\n",
      "positive\n",
      "\n",
      "Why is #Google advertising Adwords Express on #Bing? Hmmm, I know Ballmer is happy :-D. #Google, your already a monopoly.\n",
      "negative\n",
      "\n",
      "@RIM you made it too easy for me to switch to @Apple iPhone. See ya!\n",
      "positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"labeled_corpus.tsv\", encoding=\"utf-8\") as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter='\\t')\n",
    "    countdown = 5\n",
    "    \n",
    "    for row in readCSV:\n",
    "        line_arr = list(row)\n",
    "\n",
    "        tweet = line_arr[0]\n",
    "        sentiment = line_arr[1]        \n",
    "                 \n",
    "        print (tweet)\n",
    "        print (sentiment)\n",
    "        print()\n",
    "        \n",
    "        countdown-=1\n",
    "        if countdown==0: \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd44b83",
   "metadata": {},
   "source": [
    "We see that some tweets are labeled as 'positive' and others as 'negative'. There are overall four different categories of documents (tweets) in this dataset. Let's store these categories in a set. We can also make use of categories as numbers or as indexes in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2a671d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'positive', 'negative', 'neutral', 'irrelevant'}\n",
    "category_to_num = {\"positive\": 0, \"negative\":1, \"neutral\":2, \"irrelevant\": 3}\n",
    "num_to_category = {0: \"positive\", 1: \"negative\", 2: \"neutral\", 3: \"irrelevant\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1fec21",
   "metadata": {},
   "source": [
    "## Cleaning tweets\n",
    "The main characteristic of a single tweet is that unlike other documents, the tweet is short (limited by 140 characters) and contains a lot of shortcuts and emoticons. We need to carefully preserve every single word, unlike in normal natural language documents, where we would remove some words (stop words). It is probably wise to remove words starting with @ as they represent user names. It is also desirable to strip the hashtag from the word it is attached to.\n",
    "\n",
    "To take into account all abbreviations and emoticons, we replace the most obvious of them with the words which may help us with sentiment analysis. The range of frequent emoticons and their replacement is defined in emo_repl dictionary below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb57548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_repl = {\n",
    "    # positive emoticons\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",  # :D in lower case\n",
    "    \":dd\": \" good \", # :DD in lower case\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    # negative emoticons:\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":S\": \" bad \",\n",
    "    \":-S\": \" bad \",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d6601",
   "metadata": {},
   "source": [
    "Here are also the most common abbreviations to be replaced with actual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd94829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expressions module\n",
    "\n",
    "# Define abbreviations\n",
    "# as regular expressions together with their expansions\n",
    "# (\\b marks the word boundary):\n",
    "re_repl = {\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c2eb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that e.g. :dd is replaced before :d\n",
    "emo_repl_order = [k for (k_len,k) in\n",
    "                  reversed(sorted([(len(k),k) for k in emo_repl.keys()]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d4af6",
   "metadata": {},
   "source": [
    "And here is a function that will apply all of the above to a single tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c066e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    for k in emo_repl_order:\n",
    "        tweet = tweet.replace(k, emo_repl[k])\n",
    "\n",
    "    for r, repl in re_repl.items():\n",
    "        tweet = re.sub(r, repl, tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040af0e",
   "metadata": {},
   "source": [
    "## Parsing and counting words\n",
    "To work with a tweet as a document, we first need to obtain separate words from a tweet text. \n",
    "\n",
    "Because the same word rarely occurs in a tweet, we are going to account for the presence/absence of a word rather than the count of words per tweet. \n",
    "\n",
    "**Stop and think:** words with which prefix should you ignore when parsing a tweet? (#, @?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e354e",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:yellow;\">\n",
    "    <h3>Task 1. Count words in each category</h3>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4eb30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def split_tweet(string):\n",
    "    split = string.split()\n",
    "    output = []\n",
    "    for w in split:\n",
    "        if '#' not in w and '@' not in w:\n",
    "            str = \"\"\n",
    "            for ch in w:\n",
    "                if (ch not in exclude):\n",
    "                    str += ch\n",
    "            output += [str]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19e0fc",
   "metadata": {},
   "source": [
    "The probabilities of  $P(Word_i = true | Category = c)$ can be easily computed from the dataset as the number of documents in category $c$ which contain this word / total number of documents in this category (conditional probability). $P(Category=c)$ is simply the number of all documents in this category divided by the total number of documents in the dataset.\n",
    "\n",
    "Based on all the information given above, read the dataset, clean each tweet, remove some prefixed words, and split the tweet into words using the function `split_tweet()` defined above.\n",
    "\n",
    "Then for each word from an obtained list generate the following counts: \n",
    "- Number of tweets in category \"positive\" that contain this word  \n",
    "- Number of tweets in category \"negative\" that contain this word  \n",
    "- Number of tweets in category \"neutral\" that contain this word   \n",
    "- Number of tweets in category \"irrelevant\" that contain this word   \n",
    "\n",
    "Also count the total number of tweets in each category and the overall total number of tweets.\n",
    "\n",
    "Implement this inside the routine called `produce_counts`. Add counts to the corresponding entries of `word_counts_dict` and `total_entries`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57960897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_counts(word_counts_dict, total_entries):\n",
    "    \n",
    "    filepath = \"labeled_corpus.tsv\"\n",
    "\n",
    "    with open(filepath, encoding=\"utf-8\") as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter='\\t')\n",
    "        \n",
    "        for row in readCSV:\n",
    "            tweet, sentiment = row\n",
    "            \n",
    "            # clean and split tweet\n",
    "            cleaned_tweet = clean_tweet(tweet)\n",
    "            words = split_tweet(cleaned_tweet)\n",
    "\n",
    "            category_index = category_to_num[sentiment]\n",
    "\n",
    "            for word in words:\n",
    "                if word not in word_counts_dict:\n",
    "                    word_counts_dict[word] = [0, 0, 0, 0, 0]\n",
    "                \n",
    "                word_counts_dict[word][category_index] += 1\n",
    "                word_counts_dict[word][4] += 1\n",
    "            \n",
    "            total_entries[category_index] += 1\n",
    "            total_entries[-1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87d7971d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total positives = 519\n",
      "total negatives = 572\n",
      "total documents = 5107\n",
      "[6, 8, 12, 0, 26]\n",
      "[12, 6, 16, 0, 34]\n"
     ]
    }
   ],
   "source": [
    "word_counts_dict = {} # \"word\": [positive, negative, neutral, irrelevant, total counts]\n",
    "total_entries = [0, 0, 0, 0, 0] #[positive, negative, neutral, irrelevant, total counts]\n",
    "\n",
    "produce_counts(word_counts_dict, total_entries)\n",
    "\n",
    "##### Double-check that your counts are correct\n",
    "# total positives = 519\n",
    "print (\"total positives =\", total_entries[0])\n",
    "\n",
    "# total negatives = 572\n",
    "print (\"total negatives =\", total_entries[1])\n",
    "\n",
    "# total documents = 5107\n",
    "print (\"total documents =\", total_entries[-1])\n",
    "\n",
    "#'made': [6, 8, 12, 0, 26]\n",
    "print(word_counts_dict['made'])\n",
    "#'thank': [12, 6, 16, 0, 34]\n",
    "print(word_counts_dict['thank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf90ad",
   "metadata": {},
   "source": [
    "## Build Naive Bayes\n",
    "\n",
    "It is time to build a classifier - that is to precompute: for each category $c$ the $P(Category = c)$, and for each $Word_i$ the value of  $P(Word_i|c)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2313dbd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow;\">\n",
    "    <h3>Task 2. Compute probabilities</h3>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba20c2",
   "metadata": {},
   "source": [
    "Store the probability for each word in a dictionary `probabilities_dict` where the key is the word itself, and the value is a list of probabilities of this word occuring in 4 different categories. Store prior probabilities of $P(c)$ in `prior_probabilities` list with a separate entry for each category. \n",
    "\n",
    "Use counts obtained in the previous section and implement the `probabilities()` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8e05703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities(prior_probabilities, probabilities_dict):\n",
    "     \n",
    "     total_docs = total_entries[-1]\n",
    "     \n",
    "     for i in range(len(total_entries) - 1):\n",
    "        prior_probabilities[i] = total_entries[i] / total_docs\n",
    "    \n",
    "     for word, counts in word_counts_dict.items():\n",
    "          probabilities_dict[word] = []\n",
    "          \n",
    "          for i in range(4): # for each category\n",
    "               word_probability = counts[i] / total_entries[i]\n",
    "               probabilities_dict[word].append(word_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6cd706",
   "metadata": {},
   "source": [
    "Now run the function and double-check that your probabilities are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5977f42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('positive') 0.10162522028588213\n",
      "P('negative') 0.11200313295476796\n",
      "P('neutral') 0.4558449187389857\n",
      "P('irrelevant') 0.3305267280203642\n",
      "[0.011560693641618497, 0.013986013986013986, 0.005154639175257732, 0.0]\n",
      "[0.023121387283236993, 0.01048951048951049, 0.006872852233676976, 0.0]\n"
     ]
    }
   ],
   "source": [
    "prior_probabilities = [0,0,0,0] #probabilities of 'positive', 'negative', 'neutral', 'irrelevant'\n",
    "probabilities_dict = {} #key: word, value array [positive, negative, neutral, irrelevant]\n",
    "\n",
    "probabilities(prior_probabilities, probabilities_dict)\n",
    "\n",
    "print(\"P('positive')\", prior_probabilities[0])     # 0.10162522028588213\n",
    "print(\"P('negative')\", prior_probabilities[1])     # 0.11200313295476796\n",
    "print(\"P('neutral')\", prior_probabilities[2])      # 0.4558449187389857\n",
    "print(\"P('irrelevant')\", prior_probabilities[3])   # 0.3305267280203642 \n",
    "\n",
    "print(probabilities_dict['made'])   #[0.011560693641618497, 0.013986013986013986, 0.005154639175257732, 0.0]\n",
    "print(probabilities_dict['thank'])  #[0.023121387283236993, 0.01048951048951049, 0.006872852233676976, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2dc365",
   "metadata": {},
   "source": [
    "## Classify geo-tagged tweets\n",
    "\n",
    "The second dataset we are using in this lab is the dataset of geo-tagged tweets containing the latitude and the longitude of an author: [geodata.txt](https://drive.google.com/file/d/1u6fmbjCdsP9AA7z1tkYfLVSMFlDFW-nJ/view?usp=sharing). \n",
    "\n",
    "This dataset comes from the paper: “A Latent Variable Model for Geographic Lexical Variation”  \n",
    "by Jacob Eisenstein, Brendan O'Connor, Noah A. Smith, and Eric P. Xing,  \n",
    "In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Cambridge, MA, 2010.\n",
    " \n",
    "For this dataset, we want to predict a category label \n",
    "of each tweet using probabilities computed in the previous step. \n",
    "\n",
    "We cannot use this dataset directly -- we need to redistribute data points into small rectangular areas of the map.\n",
    "This should be done because Google maps API would not be able to handle all our data points due to the memory and processing power constraints of the browser. Instead, we assign each data point to a small rectangular area of 0.05 x 0.05 degrees in size, and then we compute a single score for each such area to plot the center of the area as a representative of several data points. Note that this step is already completed for you: in the [geo-twits_squares.csv](https://drive.google.com/file/d/1JDEUPGqxGvt9_r8YfQ1yZM_VuQawhHEf/view?usp=sharing) the latitude and longitude represent the upper left corner of a rectangular area to which each original tweet in geo_data.txt belongs. You can download *geo-twits_squares.csv* and place it into the same directory where you store your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4785c37",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow;\">\n",
    "    <h3>Task 3. Classify geo-tweets</h3>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49645a67",
   "metadata": {},
   "source": [
    "For each twit in the [geo_twits_squares.tsv](https://drive.google.com/file/d/1JDEUPGqxGvt9_r8YfQ1yZM_VuQawhHEf/view?usp=sharing), compute probabilities of it belonging to one of 4 categories - positive, negative, neutral or irrelevant - and choose the category which is most probable. Perform the same cleaning and parsing procedures as before before computing these classes.\n",
    "\n",
    "Because the formula for classification contains a lengthy sequence of multiplications, and the multiplied probabilities are all smaller than 1, to prevent a possible numerical underflow,  we should use the log of probability instead, and compare the logs (the logs will be negative, but we still can compare them):\n",
    "\n",
    "$c_{best} = argmax(c \\in C) [log P(Category = c) + \\sum_{i=1}^n log P(Word_i = true | Category = c)]$\n",
    "\n",
    "At the end of this step, you should have a file **locations_classified.tsv**, where for each tweet you keep 3 columns: latitude, longitude, and the most probable class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177a73c",
   "metadata": {},
   "source": [
    "It might take some time to run the classifier, so you might consider using the `tqdm` Python library which will show the progress, but you do not have to. AN example of its usage is shown in the cell below (this cell is not runnable)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a908164f",
   "metadata": {},
   "source": [
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "for row in tqdm(readTSV):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92841f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def classify(input_file, output_file, result):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file, \\\n",
    "         open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "\n",
    "        for row in reader:\n",
    "            latitude, longitude, tweet = row\n",
    "            cleaned_tweet = clean_tweet(tweet)\n",
    "            words = split_tweet(cleaned_tweet)\n",
    "            \n",
    "            category_scores = [0,0,0,0]\n",
    "            \n",
    "            for word in words:\n",
    "                if word in probabilities_dict:\n",
    "                    for i in range(4):\n",
    "                        if probabilities_dict[word][i] <= 0:\n",
    "                            category_scores[i] *= 1 # don't change anything, basically\n",
    "                        else:\n",
    "                            category_scores[i] += log(probabilities_dict[word][i])\n",
    "                else:\n",
    "                    for i in range(4):\n",
    "                        category_scores[i] += log(prior_probabilities[i])\n",
    "                        \n",
    "            for i in range(4):\n",
    "                category_scores[i] += log(prior_probabilities[i])\n",
    "                        \n",
    "            most_probable_category = category_scores.index(max(category_scores))\n",
    "\n",
    "            result.append([latitude, longitude, most_probable_category])\n",
    "            \n",
    "            writer.writerow([latitude, longitude, most_probable_category])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09080a",
   "metadata": {},
   "source": [
    "Now you can run this classification routine, which might take some time to finish. \n",
    "\n",
    "If for each tweet you also add the result \\[lat, long, class\\] to the results array then the first 5 entries in this array should look like:\n",
    "`[['40.2', '-74.85', 3], ['40.2', '-74.85', 1], ['40.2', '-74.85', 0], ['40.2', '-74.85', 2], ['40.2', '-74.85', 3]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05f03a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['40.2', '-74.85', 3], ['40.2', '-74.85', 1], ['40.2', '-74.85', 0], ['40.2', '-74.85', 2], ['40.2', '-74.85', 3]]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "classify('geo_twits_squares.tsv', 'locations_classified.tsv', result)\n",
    "print(result[0:5])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf0103e",
   "metadata": {},
   "source": [
    "## Positivity score\n",
    "Here is where some creativity is needed.\n",
    "\n",
    "Now that we can count the number of total tweets per area, the number of positive twits and the number of negative twits in it, how would you assign a single positivity score to each area?\n",
    "\n",
    "The goal is to have a single score which reflects both the proportion of positive and the proportion of negative tweets, and we want this score to range from 0 to 1.  We want the score >=0.5 to indicate an overall positive spirit of the area, and the score < 0.5 its negative spirit.\n",
    "\n",
    "Here is one possible suggestion:\n",
    "\n",
    "Consider the following example: \n",
    "There are a total of 10 tweets in the area.\n",
    "5 of them are positive, 3 are negative, 2 irrelevant/neutral.\n",
    "Positivity = total_positive/total - total_negative/total = 5/10 - 2/10 = 3/10 > 0\n",
    "\n",
    "Let’s say in another square:\n",
    "2 are positive, 7 are negative, and total is 10.\n",
    "Positivity = 2/10 - 7/10 = -5/10 < 0.\n",
    "\n",
    "Because the positivity score defined above varies from -1 to 1, and we want it to vary from 0 to 1, we could just add 1 and divide by 2 - and we get the score in the desired interval.\n",
    "\n",
    "You can think of a different score, or you can make changes in the visualization code to accommodate the different score interval. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce8672",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow;\">\n",
    "    <h3>Task 4. Compute positivity scores</h3>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da5a9d",
   "metadata": {},
   "source": [
    "For each rectangular geographic area compute a single positivity score.\n",
    "\n",
    "The output from this step is a dataset where for each area we have its left upper corner (latitude, longitude) and the average positivity score. You may store this dataset in file **positivity_score.tsv**. For us to be able to test your work, please  print several scores here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "79868cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positivity_score(input_file, output_file, result):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file, \\\n",
    "         open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "        \n",
    "        area_counts = {}\n",
    "        \n",
    "        for row in reader:\n",
    "            latitude, longitude, sentiment = row\n",
    "            key = (latitude, longitude)\n",
    "            \n",
    "            if key not in area_counts:\n",
    "                area_counts[key] = {'positive': 0, 'negative': 0, 'total': 0}\n",
    "            \n",
    "            if sentiment == '0':\n",
    "                area_counts[key]['positive'] += 1\n",
    "            elif sentiment == '1':\n",
    "                area_counts[key]['negative'] += 1\n",
    "            area_counts[key]['total'] += 1\n",
    "        \n",
    "        for key, counts in area_counts.items():\n",
    "            if counts['total'] > 0:\n",
    "                positivity = (counts['positive'] / counts['total']) - (counts['negative'] / counts['total'])\n",
    "                positivity = (positivity + 1) / 2\n",
    "            else:\n",
    "                positivity = 0.5\n",
    "            \n",
    "            result.append([key[0], key[1], positivity])\n",
    "            writer.writerow([key[0], key[1], positivity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "11fc2859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['40.2', '-74.85', 0.6071428571428572],\n",
       " ['40.2', '-74.75', 0.5108695652173914],\n",
       " ['40.2', '-74.8', 0.5343137254901961],\n",
       " ['40.15', '-74.8', 0.5],\n",
       " ['40.25', '-74.7', 0.25]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positivity = []\n",
    "positivity_score('locations_classified.tsv','positivity_score.tsv', positivity)\n",
    "positivity[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e5306",
   "metadata": {},
   "source": [
    "## Visualizing results\n",
    "Now your goal is to visualize positivity on the map. You can learn how to create your own map visualizations [here](). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596307dd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow;\">\n",
    "    <h3>Task 5. Create visualization input</h3>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46694f",
   "metadata": {},
   "source": [
    "If you want to use the provided demo template, you would need to create a javascript variable `data` in JSON format and write this object to file `data.js` (located in public_html folder) as following:\n",
    "\n",
    "`var data =[{\"score\": 0, \"g\": -122.197916, \"t\": 47.528139}, {\"score\": 0.5, \"g\": -122.197916, \"t\": 47.528139}, … ];`\n",
    "\n",
    "As you see, `data` is an array of Javascript ‘objects’, and each object contains 3 attributes: ‘score’ of the area, ‘g’ for longitude and ‘t’ for latitude. Do not forget to add 0.05/2 to each coordinate to represent the center of the rectangular area rather than its left upper corner.\n",
    "\n",
    "Dump your resulting object to file `data.js`, place this file into the public_html folder, and explore the visualization by opening index.html in your browser. \n",
    "\n",
    "What areas are most positive, which ones are most negative?  Write down your observations in a new cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "52379ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_js_data(input_file, output_file):\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            latitude, longitude, score = row\n",
    "            latitude = str(float(latitude) + 0.025)\n",
    "            longitude = str(float(longitude) + 0.025)\n",
    "            score = float(score)\n",
    "            data.append({\"score\": score, \"g\": float(longitude), \"t\": float(latitude)})\n",
    "    \n",
    "    data_json = json.dumps(data)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"var data ={data_json};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65216f19",
   "metadata": {},
   "source": [
    "Run it and test if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3573984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_js_data('positivity_score.tsv', 'public_html/data.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c3984",
   "metadata": {},
   "source": [
    "## Before you submit\n",
    "Check that everything still works as expected if you call all your functions one after another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_dict = {} # \"word\": [positive, negative, neutral, irrelevant, total counts]\n",
    "total_entries = [0, 0, 0, 0, 0] #[positive, negative, neutral, irrelevant, total counts]\n",
    "\n",
    "produce_counts() #count tweets in training data\n",
    "\n",
    "prior_probabilities = [0,0,0,0] #probabilities of 'positive', 'negative', 'neutral', 'irrelevant'\n",
    "probabilities_dict = {} #key: word, value array [positive, negative, neutral, irrelevant]\n",
    "\n",
    "probabilities() #calculate probabilities\n",
    "classify() #classify and write locations_classified.tsv\n",
    "positivity_score() #calculate positivity scores and write positivity_score.tsv\n",
    "generate_js_data() #write data.js"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd46f2",
   "metadata": {},
   "source": [
    "## Submit your work by committing the forked repository to github. \n",
    "*Do not commit any data files: they are too big for github*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a388f8",
   "metadata": {},
   "source": [
    "### This is the end of the Skill lab 3. \n",
    "\n",
    "Copyright &copy; 2024 Marina Barsky."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
